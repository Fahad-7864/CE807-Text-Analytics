{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Student ID: 22022\n",
        "\n",
        "**You student_id is your 7/8 digit faser number.** \n",
        "\n",
        "This is a sample format for CE807: Assignment 2. You must follow the format.\n",
        "The code will have three broad sections, and additional section, if needed,\n",
        "\n",
        "\n",
        "1.   Common Codes\n",
        "2.   Method/model 1 Specific Codes\n",
        "3.   Method/model 2 Specific Codes\n",
        "4.   Other Method/model Codes, if any\n",
        "\n",
        "You code should be proverly indended, print as much as possible, follow standard coding (https://peps.python.org/pep-0008/) and documentaion (https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb) practices. \n",
        "\n",
        "Before each `code cell`, you must have a `text cell` which explain what code cell is going to do. For each function/class, you need to properly document what are it's input, functionality and output. \n",
        "\n",
        "If you are using any non-standard library, you must have command to install that, for example `pip install datasets`. \n",
        "\n",
        "You must print `train`, `validation` and `test` performance measures.\n",
        "\n",
        "You must also print `train` and `validation` loss in each `epoch`, wherever you are using `epoch`, say in any deep learning algorithms.\n",
        "\n",
        "Your code must\n",
        "\n",
        "*   To reproducibality of the results you must use a `seed`, you have to set seed in `torch`, `numpy` etc, use same seed everywhere **and your Student ID should be your seed**. \n",
        "*   read dataset from './CE807/Assignment2/student_id/' folder which will have 3 files [`train.csv`, `val.csv`, `test.csv`]\n",
        "*   save model after finishing the training in './CE807/Assignment2/student_id/models/XXX/' where XXX = [1,2] for both models\n",
        "*   at testing time you will load models from './CE807/Assignment2/student_id/models/XXX/' where XXX = [1,2] and then test on your data, and save the output in the same folder\n",
        "*   For Data Size Effect, you model and output save directories are './CE807/Assignment2/student_id/models/XXX/YYY/' where XXX = [1,2] and YYY = [25,50, 75,100]\n",
        "*   **Your output file based on the test file will be named `output_test.csv` and will have fields `id`, `tweet`, `label` and `out_label`** Note that, `id`, `tweet`, `label` come from `test.csv`, and `out_label` out_label your model’s output, where out_label =[OFF,NOT]. You need to save file in the respective model folders. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Install and import all required libraries first before starting to code.**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VgzEm1gDYBUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install all require libraries. For example, `transformers`"
      ],
      "metadata": {
        "id": "_3ZWJlO6JOqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "bqxPHsUOqVvh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c733345-e162-4a85-bc2d-ef2aaa8c7c42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.0 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import all require libraries. \n",
        "For example, `numpy`"
      ],
      "metadata": {
        "id": "U5XEt6asIi3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import numpy\n",
        "import io \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "\n",
        "import pickle "
      ],
      "metadata": {
        "id": "TKEZRYhIImbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's put your student id as a variable, that you will use different places**"
      ],
      "metadata": {
        "id": "pd5kSsAPZoE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_id = 220222 # Note this is an interger and you need to input your id"
      ],
      "metadata": {
        "id": "rqP6pp_3ZkVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's set `seed` for all libraries like `torch`, `numpy` etc as my student id"
      ],
      "metadata": {
        "id": "RiLUrQ-3zC6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set same seeds for all libraries\n",
        "\n",
        "#numpy seed\n",
        "np.random.seed(student_id)"
      ],
      "metadata": {
        "id": "TYUn2tj3zCFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "L1kvIe1NbDoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d40f8e3-89e7-4a80-eaa1-dfa473167377"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add your code to initialize GDrive and data and models paths\n",
        "\n",
        "# TODO: Fill in the Google Drive path where you uploaded the assignment, data and code\n",
        "# Example: If your student_id is 1234567 then your directory will be './CE807/Assignment2/1234567/' \n",
        "\n",
        "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = os.path.join('./CE807/Assignment2/',str(student_id)) # Make sure to update with your student_id and student_id is an integer\n",
        "GOOGLE_DRIVE_PATH = os.path.join('gdrive', 'MyDrive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "print('List files: ', os.listdir(GOOGLE_DRIVE_PATH))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WKnbP3roLTj",
        "outputId": "a55d80dc-0c93-433f-cc2c-32223fee5303"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "List files:  ['test.csv', 'train.csv', 'valid.csv', 'models', 'train_25.csv', 'CE807- Assignment 2.ipynb', 'train_50.csv', 'train_75.csv', 'train_100.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Let's get all file names"
      ],
      "metadata": {
        "id": "DdLem9mzWDQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv') # This is 100% of data\n",
        "train_25_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv') #Let's assume that you have train 25% file is saved in train_25.csv. Note that this is a dummy file. You have to create your own file.\n",
        "train_50_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv')\n",
        "train_75_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv')\n",
        "train_100_file = os.path.join(GOOGLE_DRIVE_PATH, 'train_100.csv')\n",
        "val_file = os.path.join(GOOGLE_DRIVE_PATH, 'valid.csv')\n",
        "test_file = os.path.join(GOOGLE_DRIVE_PATH, 'test.csv')\n",
        "\n",
        "print('Train 100% file: ', train_100_file)\n",
        "print('Train 25% file: ', train_25_file)\n",
        "print('Train 50% file: ', train_50_file)\n",
        "print('Train 75% file: ', train_75_file)\n",
        "\n",
        "print('Validation file: ', val_file)\n",
        "print('Test file: ', test_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaCVLw5xV-5A",
        "outputId": "29f9617a-2177-4f55-e5c6-6513622bd4d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train 100% file:  gdrive/MyDrive/./CE807/Assignment2/220222/train_100.csv\n",
            "Train 25% file:  gdrive/MyDrive/./CE807/Assignment2/220222/train_25.csv\n",
            "Train 50% file:  gdrive/MyDrive/./CE807/Assignment2/220222/train_50.csv\n",
            "Train 75% file:  gdrive/MyDrive/./CE807/Assignment2/220222/train_75.csv\n",
            "Validation file:  gdrive/MyDrive/./CE807/Assignment2/220222/valid.csv\n",
            "Test file:  gdrive/MyDrive/./CE807/Assignment2/220222/test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set output model directory and file names\n",
        "MODEL_1_DIRECTORY = os.path.join(GOOGLE_DRIVE_PATH, 'models', '1')\n",
        "MODEL_1_25_DIRECTORY = os.path.join(MODEL_1_DIRECTORY, '25')\n",
        "MODEL_1_100_DIRECTORY = os.path.join(MODEL_1_DIRECTORY, '100')\n",
        "\n",
        "print('Model 1 directory: ', MODEL_1_DIRECTORY)\n",
        "print('Model 1 directory with 25% data: ', MODEL_1_25_DIRECTORY)\n",
        "print('Model 1 directory with 100% data: ', MODEL_1_100_DIRECTORY)\n",
        "\n",
        "model_1_25_output_test_file = os.path.join(MODEL_1_25_DIRECTORY, 'output_test.csv')\n",
        "model_1_100_output_test_file = os.path.join(MODEL_1_100_DIRECTORY, 'output_test.csv')\n",
        "\n",
        "print('Output file name using model 1 using 25% of train data: ', model_1_25_output_test_file)\n",
        "print('Output file name using model 1 using 100% of train data: ', model_1_100_output_test_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQiwiE7BWXyQ",
        "outputId": "d5c5ca03-5501-42c7-a63d-7bb2514beb43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model 1 directory:  /content/gdrive/MyDrive/CE807/Assignment2/220222/models/1\n",
            "Model 1 directory with 25% data:  /content/gdrive/MyDrive/CE807/Assignment2/220222/models/1/25\n",
            "Model 1 directory with 100% data:  /content/gdrive/MyDrive/CE807/Assignment2/220222/models/1/100\n",
            "Output file name using model 1 using 25% of train data:  /content/gdrive/MyDrive/CE807/Assignment2/220222/models/1/25/output_test.csv\n",
            "Output file name using model 1 using 100% of train data:  /content/gdrive/MyDrive/CE807/Assignment2/220222/models/1/100/output_test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### SECTION 3.\n",
        "###Adding all the splits to the data."
      ],
      "metadata": {
        "id": "LfW_3TVAQqhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "def split_data(data):\n",
        "    cumulative_data_splits = []\n",
        "    remaining_data = data.copy()\n",
        "    train_sizes = [0.25, 0.5, 0.75, 1]\n",
        "\n",
        "    for i in range(4):\n",
        "        if i == 0:\n",
        "            train_data, remaining_data = train_test_split(\n",
        "                remaining_data,\n",
        "                train_size=train_sizes[i],\n",
        "                random_state=42,\n",
        "                stratify=remaining_data['label'],\n",
        "            )\n",
        "        else:\n",
        "            current_train_size = train_sizes[i] - train_sizes[i - 1]\n",
        "            train_data, remaining_data = train_test_split(\n",
        "                remaining_data,\n",
        "                train_size=current_train_size,\n",
        "                random_state=42,\n",
        "                stratify=remaining_data['label'],\n",
        "            )\n",
        "\n",
        "        if i == 0:\n",
        "            cumulative_data_splits.append(train_data)\n",
        "        else:\n",
        "            cumulative_data_splits.append(pd.concat([cumulative_data_splits[-1], train_data]))\n",
        "\n",
        "    return cumulative_data_splits\n",
        "\n",
        "\n",
        "GOOGLE_DRIVE_PATH = '/content/gdrive/MyDrive/CE807/Assignment2/220222'\n",
        "train_file = os.path.join(GOOGLE_DRIVE_PATH, 'train.csv')  # This is 100% of data\n",
        "\n",
        "# Read the data from the file\n",
        "train_data = pd.read_csv(train_file)\n",
        "\n",
        "# Generate the four cumulative dataset splits\n",
        "cumulative_data_splits = split_data(train_data)\n",
        "\n",
        "# Save the first dataset (25% of the dataset) as a CSV file in the specified directory\n",
        "cumulative_data_splits[0].to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_25.csv'), index=False)\n",
        "cumulative_data_splits[1].to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_50.csv'), index=False)\n",
        "cumulative_data_splits[2].to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_75.csv'), index=False)\n",
        "cumulative_data_splits[3].to_csv(os.path.join(GOOGLE_DRIVE_PATH, 'train_100.csv'), index=False)\n"
      ],
      "metadata": {
        "id": "tmIQUZADLD_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N2dgfxvAWX2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see train file"
      ],
      "metadata": {
        "id": "0pWvXDghtafa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(train_file)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "dptNCuqjq5k_",
        "outputId": "3a9ff4c1-3a9b-4cff-9e28-ac38e3fe7c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id                                              tweet label\n",
              "0  42884  @USER I’m done with you as well. An INTENTIONA...   NOT\n",
              "1  92152  I now have over 6k followers.  Only 94k to go ...   NOT\n",
              "2  65475  @USER Tom was bought! He is more interested in...   NOT\n",
              "3  22144  @USER @USER Even her brother thinks she is a m...   OFF\n",
              "4  81048  @USER @USER @USER @USER @USER I can understand...   OFF"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d795e15b-570b-4552-ad70-a20f29278927\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>42884</td>\n",
              "      <td>@USER I’m done with you as well. An INTENTIONA...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92152</td>\n",
              "      <td>I now have over 6k followers.  Only 94k to go ...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>65475</td>\n",
              "      <td>@USER Tom was bought! He is more interested in...</td>\n",
              "      <td>NOT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22144</td>\n",
              "      <td>@USER @USER Even her brother thinks she is a m...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>81048</td>\n",
              "      <td>@USER @USER @USER @USER @USER I can understand...</td>\n",
              "      <td>OFF</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d795e15b-570b-4552-ad70-a20f29278927')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d795e15b-570b-4552-ad70-a20f29278927 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d795e15b-570b-4552-ad70-a20f29278927');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zjoEyEIwLDI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Uncomment code to see the statistics of the val,test and train files.\n",
        "#df_val = pd.read_csv(val_file)\n",
        "#df_test = pd.read_csv(test_file)\n",
        "#from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Printing dataset statistics\n",
        "print(\"Training Set Statistics:\")\n",
        "print(df[\"label\"].value_counts())\n",
        "#print(\"\\nValidation Set Statistics:\")\n",
        "#print(df_val[\"label\"].value_counts())\n",
        "#print(\"\\nTest Set Statistics:\")\n",
        "#print(df_test[\"label\"].value_counts())\n",
        "\n",
        "# Printing average tweet length\n",
        "train_tweet_lengths = df[\"tweet\"].apply(len)\n",
        "print(\"\\nAverage Tweet Length in Training Set:\", np.mean(train_tweet_lengths))\n",
        "#val_tweet_lengths = df_val[\"tweet\"].apply(len)\n",
        "#print(\"Average Tweet Length in Validation Set:\", np.mean(val_tweet_lengths))\n",
        "#test_tweet_lengths = df_test[\"tweet\"].apply(len)\n",
        "#print(\"Average Tweet Length in Test Set:\", np.mean(test_tweet_lengths))\n",
        "# Printing number of unique words\n",
        "\n",
        "# Create a CountVectorizer object\n",
        "#vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the training set\n",
        "#vectorizer.fit(df[\"tweet\"])\n",
        "\n",
        "# Get the number of unique words in each set\n",
        "#train_unique_words = len(vectorizer.vocabulary_)\n",
        "#val_unique_words = len(vectorizer.transform(df_val[\"tweet\"]).toarray().sum(axis=0).nonzero()[0])\n",
        "#test_unique_words = len(vectorizer.transform(df_test[\"tweet\"]).toarray().sum(axis=0).nonzero()[0])\n",
        "\n",
        "#print(\"Number of unique words in training set:\", train_unique_words)\n",
        "#print(\"Number of unique words in validation set:\", val_unique_words)\n",
        "#print(\"Number of unique words in test set:\", test_unique_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBZSX5HL5hwB",
        "outputId": "9fe93d52-8a5f-4e06-f405-da9b5ef73c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Set Statistics:\n",
            "NOT    8221\n",
            "OFF    4092\n",
            "Name: label, dtype: int64\n",
            "\n",
            "Average Tweet Length in Training Set: 126.32640298871112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use different performance matrics like Accuracy, Recall (macro), Precision (macro), F1 (macro) and Confusion Matrix for the performance evaluation. We will print all the matrics and display Confusion Matrix with proper X & Y axis labels"
      ],
      "metadata": {
        "id": "E-OjJ4REbhcj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method 1 Start\n",
        "\n",
        "In this section you will write all details of your Method 1. \n",
        "\n",
        "You will have to enter multiple `code` and `text` cell.\n",
        "\n",
        "Your code should follow the standard ML pipeline\n",
        "\n",
        "\n",
        "*   Data reading\n",
        "*   Data clearning, if any\n",
        "*   Convert data to vector/tokenization/vectorization\n",
        "*   Model Declaration/Initialization/building\n",
        "*   Training and validation of the model using training and validation dataset \n",
        "*   Save the trained model\n",
        "*   Load and Test the model on testing set\n",
        "*   Save the output of the model\n",
        "\n",
        "\n",
        "You could add any other step(s) based on your method's requirement. \n",
        "\n",
        "After finishing the above, you need to usd splited data as defined in the assignment and then do the same for all 4 sets. Your code should not be copy-pasted 4 time, make use of `function`.\n"
      ],
      "metadata": {
        "id": "47ywe8jGSKhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Z-bthsb3SX6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a486d2b-18c8-4b67-8729-8c78d6514793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This function preprocesses the input text by removing stop words and lowercasing all words.\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = word_tokenize(text)\n",
        "    filtered_words = [word.lower() for word in words if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_words)\n"
      ],
      "metadata": {
        "id": "fQfnLzAfJCV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function reads the data from a file, shuffles it, and preprocesses the text.\n",
        "def prepare_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\n",
        "    data['tweet'] = data['tweet'].apply(preprocess_text)\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "6aw6Rb6FJC3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(data, tokenizer=None):\n",
        "    if tokenizer is None:\n",
        "        tokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\n",
        "        tokenizer.fit_on_texts(data['tweet'])\n",
        "    sequences = tokenizer.texts_to_sequences(data['tweet'])\n",
        "    padded = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')\n",
        "    return padded, tokenizer\n"
      ],
      "metadata": {
        "id": "J6cnOH7SJDDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lstm_model(embedding_size=128, lstm_units=64, dense_units=64, dropout_rate=0.4, optimizer='adam'):\n",
        "    \"\"\"\n",
        "    Create an LSTM model with the specified parameters and compile it.\n",
        "    \"\"\"\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(10000, embedding_size, input_length=100),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units // 2)),\n",
        "        tf.keras.layers.Dense(dense_units, activation='relu'),\n",
        "        tf.keras.layers.Dropout(dropout_rate),\n",
        "        tf.keras.layers.Dense(dense_units // 2, activation='relu'),\n",
        "        tf.keras.layers.Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "egj151ytJNtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T38qE9DgPTUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method1(model, train_padded, train_labels, val_padded, val_labels, epochs=7, batch_size=32, sample_weights=None):\n",
        "    \"\"\"\n",
        "    Train the LSTM model using the padded training data and labels.\n",
        "    \"\"\"\n",
        "    history = model.fit(train_padded, train_labels, epochs=epochs, batch_size=batch_size, verbose=2, sample_weight=sample_weights, validation_data=(val_padded, val_labels))\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} - Loss: {history.history['loss'][epoch]:.4f} - Validation Loss: {history.history['val_loss'][epoch]:.4f}\")\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "djgW-Dd5JN8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method1(model, train_padded, val_padded, test_padded, train_labels, val_labels, test_labels):\n",
        "    # Evaluate on train, validation, and test datasets\n",
        "    datasets = {'train': (train_padded, train_labels),\n",
        "                'validation': (val_padded, val_labels),\n",
        "                'test': (test_padded, test_labels)}\n",
        "\n",
        "    for dataset_name, (padded_data, labels) in datasets.items():\n",
        "        loss, accuracy = model.evaluate(padded_data, labels, verbose=1)\n",
        "        print(f\"{dataset_name.capitalize()} Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "        predicted_probs = model.predict(padded_data)\n",
        "        predicted_labels = np.argmax(predicted_probs, axis=1)\n",
        "\n",
        "        print(f\"{dataset_name.capitalize()} LSTM Classification Report:\")\n",
        "        print(classification_report(labels, predicted_labels))\n",
        "\n",
        "        print(f\"{dataset_name.capitalize()} LSTM Confusion Matrix:\")\n",
        "        print(confusion_matrix(labels, predicted_labels))\n",
        "\n",
        "        if dataset_name == 'test':\n",
        "            return accuracy, predicted_labels\n"
      ],
      "metadata": {
        "id": "JHZlBKPaJOKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def save_lstm_model(model, tokenizer, model_dir):\n",
        "    \"\"\"\n",
        "    Save the LSTM model and tokenizer to disk.\n",
        "    \"\"\"\n",
        "    model_file = os.path.join(model_dir, 'lstm_model.h5')\n",
        "    model.save(model_file)\n",
        "    print('Saved LSTM model to', model_file)\n",
        "\n",
        "    tokenizer_file = os.path.join(model_dir, 'tokenizer.pickle')\n",
        "    with open(tokenizer_file, 'wb') as handle:\n",
        "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print('Saved Tokenizer to', tokenizer_file)\n",
        "\n",
        "    return model_file, tokenizer_file"
      ],
      "metadata": {
        "id": "A67HJ6gmPsXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_lstm_model(model_file, tokenizer_file):\n",
        "    \"\"\"\n",
        "    Load the LSTM model and tokenizer from disk.\n",
        "    \"\"\"\n",
        "    model = tf.keras.models.load_model(model_file)\n",
        "    print('Loaded LSTM model from', model_file)\n",
        "\n",
        "    with open(tokenizer_file, 'rb') as handle:\n",
        "        tokenizer = pickle.load(handle)\n",
        "    print('Loaded Tokenizer from', tokenizer_file)\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "54l_p1DVPuhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = prepare_data(train_100_file)\n",
        "val_data = prepare_data(val_file)\n",
        "test_data = prepare_data(test_file)\n",
        "\n",
        "train_padded, tokenizer = tokenize_data(train_data)\n",
        "val_padded, _ = tokenize_data(val_data, tokenizer)\n",
        "test_padded, _ = tokenize_data(test_data, tokenizer)\n",
        "\n",
        "train_labels = train_data['label'].apply(lambda x: 0 if x == 'NOT' else 1).values\n",
        "val_labels = val_data['label'].apply(lambda x: 0 if x == 'NOT' else 1).values\n",
        "test_labels = test_data['label'].apply(lambda x: 0 if x == 'NOT' else 1).values\n",
        "\n",
        "sample_weights = class_weight.compute_sample_weight('balanced', train_labels)\n",
        "\n",
        "model = create_lstm_model()\n",
        "#USING THE TRAIN METHOD\n",
        "model, history = train_method1(model, train_padded, train_labels, val_padded, val_labels, sample_weights=sample_weights)\n",
        "#saving the model to the necessary directory.\n",
        "model_dir = 'gdrive/MyDrive/./CE807/Assignment2/220222/models/1/100'  # Update this to your desired directory\n",
        "model_file, tokenizer_file = save_lstm_model(model, tokenizer, model_dir)\n",
        "\n",
        "#loading the model\n",
        "loaded_model, loaded_tokenizer = load_lstm_model(model_file, tokenizer_file)\n",
        "\n",
        "#Using The Test MEthod\n",
        "test_accuracy, test_predicted_labels = test_method1(loaded_model, train_padded, val_padded, test_padded, train_labels, val_labels, test_labels)\n",
        "# Create a new DataFrame with the desired format\n",
        "output_df = test_data[['id', 'tweet', 'label']].copy()\n",
        "output_df['out_label'] = test_predicted_labels\n",
        "output_df['out_label'] = output_df['out_label'].apply(lambda x: 'NOT' if x == 0 else 'OFF')\n",
        "\n",
        "# Save the output DataFrame to a file\n",
        "output_file = os.path.join(model_dir, 'output.csv')\n",
        "output_df.to_csv(output_file, index=False)\n",
        "print(f\"Output file saved at: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IrPRT4JP9cw",
        "outputId": "f1b0a3a9-e349-4b9e-e220-45e7420ae214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/7\n",
            "263/263 - 81s - loss: 0.6476 - accuracy: 0.6280 - val_loss: 0.5974 - val_accuracy: 0.7389 - 81s/epoch - 307ms/step\n",
            "Epoch 2/7\n",
            "263/263 - 71s - loss: 0.4498 - accuracy: 0.8155 - val_loss: 0.5660 - val_accuracy: 0.7217 - 71s/epoch - 270ms/step\n",
            "Epoch 3/7\n",
            "263/263 - 67s - loss: 0.3015 - accuracy: 0.8881 - val_loss: 0.6247 - val_accuracy: 0.7141 - 67s/epoch - 256ms/step\n",
            "Epoch 4/7\n",
            "263/263 - 66s - loss: 0.1902 - accuracy: 0.9297 - val_loss: 0.9629 - val_accuracy: 0.6969 - 66s/epoch - 252ms/step\n",
            "Epoch 5/7\n",
            "263/263 - 69s - loss: 0.1230 - accuracy: 0.9544 - val_loss: 0.9648 - val_accuracy: 0.7163 - 69s/epoch - 262ms/step\n",
            "Epoch 6/7\n",
            "263/263 - 67s - loss: 0.0885 - accuracy: 0.9703 - val_loss: 1.2309 - val_accuracy: 0.7087 - 67s/epoch - 255ms/step\n",
            "Epoch 7/7\n",
            "263/263 - 67s - loss: 0.0711 - accuracy: 0.9767 - val_loss: 1.4162 - val_accuracy: 0.6926 - 67s/epoch - 255ms/step\n",
            "Epoch 1 - Loss: 0.6476 - Validation Loss: 0.5974\n",
            "Epoch 2 - Loss: 0.4498 - Validation Loss: 0.5660\n",
            "Epoch 3 - Loss: 0.3015 - Validation Loss: 0.6247\n",
            "Epoch 4 - Loss: 0.1902 - Validation Loss: 0.9629\n",
            "Epoch 5 - Loss: 0.1230 - Validation Loss: 0.9648\n",
            "Epoch 6 - Loss: 0.0885 - Validation Loss: 1.2309\n",
            "Epoch 7 - Loss: 0.0711 - Validation Loss: 1.4162\n",
            "Saved LSTM model to gdrive/MyDrive/./CE807/Assignment2/220222/models/1/100/lstm_model.h5\n",
            "Saved Tokenizer to gdrive/MyDrive/./CE807/Assignment2/220222/models/1/100/tokenizer.pickle\n",
            "Loaded LSTM model from gdrive/MyDrive/./CE807/Assignment2/220222/models/1/100/lstm_model.h5\n",
            "Loaded Tokenizer from gdrive/MyDrive/./CE807/Assignment2/220222/models/1/100/tokenizer.pickle\n",
            "263/263 [==============================] - 22s 76ms/step - loss: 0.0362 - accuracy: 0.9880\n",
            "Train Accuracy: 98.80%\n",
            "263/263 [==============================] - 19s 67ms/step\n",
            "Train LSTM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      5619\n",
            "           1       0.98      0.98      0.98      2797\n",
            "\n",
            "    accuracy                           0.99      8416\n",
            "   macro avg       0.99      0.99      0.99      8416\n",
            "weighted avg       0.99      0.99      0.99      8416\n",
            "\n",
            "Train LSTM Confusion Matrix:\n",
            "[[5571   48]\n",
            " [  53 2744]]\n",
            "29/29 [==============================] - 3s 57ms/step - loss: 1.4162 - accuracy: 0.6926\n",
            "Validation Accuracy: 69.26%\n",
            "29/29 [==============================] - 4s 83ms/step\n",
            "Validation LSTM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.74      0.76       619\n",
            "           1       0.53      0.61      0.57       308\n",
            "\n",
            "    accuracy                           0.69       927\n",
            "   macro avg       0.66      0.67      0.66       927\n",
            "weighted avg       0.70      0.69      0.70       927\n",
            "\n",
            "Validation LSTM Confusion Matrix:\n",
            "[[455 164]\n",
            " [121 187]]\n",
            "27/27 [==============================] - 3s 97ms/step - loss: 1.3804 - accuracy: 0.7244\n",
            "Test Accuracy: 72.44%\n",
            "27/27 [==============================] - 2s 74ms/step\n",
            "Test LSTM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.77      0.80       620\n",
            "           1       0.51      0.61      0.55       240\n",
            "\n",
            "    accuracy                           0.72       860\n",
            "   macro avg       0.67      0.69      0.68       860\n",
            "weighted avg       0.74      0.72      0.73       860\n",
            "\n",
            "Test LSTM Confusion Matrix:\n",
            "[[476 144]\n",
            " [ 93 147]]\n",
            "Output file saved at: gdrive/MyDrive/./CE807/Assignment2/220222/models/1/100/output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1 End\n"
      ],
      "metadata": {
        "id": "ue3xIDFGSXNH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# Method 2/model Start"
      ],
      "metadata": {
        "id": "rmaJfJkVwSDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PqIo3AOZgsD",
        "outputId": "953ffda7-5e43-4de9-882b-83173001d10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emojis = {':)': 'smile', ':(': 'frown', ':D': 'laugh', ':P': 'sticking out tongue',\n",
        "          ':O': 'surprised', ';)': 'wink', ':/': 'unsure', ':|': 'neutral', ':*': 'kiss',\n",
        "          ':$': 'embarrassed', ':@': 'angry', ':S': 'confused', '<3': 'heart', '❤️': 'heart'}\n",
        "\n",
        "def clean_data(data):\n",
        "    data = re.sub(r'@[\\w]+', '', data)  # remove @user mentions\n",
        "    data = str(data).lower()\n",
        "    data = re.sub(r\"\\d+\", \"\", data)  # remove numbers\n",
        "    data = re.sub(r\"[^\\w\\s]\", \"\", data)  # remove punctuation\n",
        "    data = re.sub(r\"\\s+\", \" \", data)  # remove extra whitespaces\n",
        "    for emoji in emojis.keys():\n",
        "        data = data.replace(emoji, emojis[emoji])\n",
        "    words = data.split()\n",
        "    return ' '.join(words)"
      ],
      "metadata": {
        "id": "5IRQaQHUZhIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    data = data.sample(frac=1).reset_index(drop=True)  # Shuffle the dataset\n",
        "    data['tweet'] = data['tweet'].apply(lambda x: clean_data(x))\n",
        "    data['label'] = data['label'].map({'OFF': 1, 'NOT': 0})\n",
        "    return data"
      ],
      "metadata": {
        "id": "R13JxkwZZhVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_voting_classifier(lr_model, rf_model, svm_model):\n",
        "    voting_clf = VotingClassifier(estimators=[('lr', lr_model), ('rf', rf_model), ('svm', svm_model)], voting='hard')\n",
        "    return voting_clf"
      ],
      "metadata": {
        "id": "rVFnjl3cZiEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_method2(train_data, train_labels):\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "    train_tfidf = vectorizer.fit_transform(train_data)\n",
        "\n",
        "    clf1 = LogisticRegression(max_iter=1000)\n",
        "    clf2 = RandomForestClassifier(n_estimators=100)\n",
        "    clf3 = SVC(kernel='linear', C=1)\n",
        "\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[('lr', clf1), ('rf', clf2), ('svm', clf3)], voting='hard'\n",
        "    )\n",
        "    voting_clf.fit(train_tfidf, train_labels)\n",
        "\n",
        "    return voting_clf, vectorizer\n"
      ],
      "metadata": {
        "id": "s211du68ZiRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_method2(voting_clf, X_test_vect, y_test):\n",
        "    y_pred_voting = voting_clf.predict(X_test_vect)\n",
        "\n",
        "    precision_voting = precision_score(y_test, y_pred_voting, average=None)\n",
        "    recall_voting = recall_score(y_test, y_pred_voting, average=None)\n",
        "    f1_voting = f1_score(y_test, y_pred_voting, average=None)\n",
        "\n",
        "    print(\"Voting Classifier Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred_voting))\n",
        "\n",
        "    print(\"Voting Classifier Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred_voting))\n"
      ],
      "metadata": {
        "id": "mFjsJZIwZi5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_voting_classifier(voting_clf, model_dir):\n",
        "    \"\"\"\n",
        "    Save the Voting Classifier to disk.\n",
        "    \"\"\"\n",
        "    model_file = os.path.join(model_dir, 'voting_classifier.pkl')\n",
        "    with open(model_file, 'wb') as f:\n",
        "        pickle.dump(voting_clf, f)\n",
        "    print('Saved Voting Classifier to', model_file)\n",
        "    return model_file"
      ],
      "metadata": {
        "id": "MUBAjIGZnr5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_voting_classifier(model_file):\n",
        "    \"\"\"\n",
        "    Load the Voting Classifier from disk.\n",
        "    \"\"\"\n",
        "    with open(model_file, 'rb') as f:\n",
        "        voting_clf = pickle.load(f)\n",
        "    print('Loaded Voting Classifier from', model_file)\n",
        "    return voting_clf"
      ],
      "metadata": {
        "id": "4tz5s2mQnsME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = train_100_file\n",
        "val_file_path = val_file\n",
        "test_file_path = test_file\n",
        "\n",
        "train_data = prepare_data(train_file_path)\n",
        "val_data = prepare_data(val_file_path)\n",
        "test_data = prepare_data(test_file_path)\n",
        "\n",
        "\n",
        "y_train = train_data['label'].values\n",
        "y_val = val_data['label'].values\n",
        "y_test = test_data['label'].values\n",
        "\n",
        "sample_weights = class_weight.compute_sample_weight('balanced', y_train)\n",
        "\n",
        "# Train the Voting Classifier and get the vectorizer\n",
        "#USING THE TRAIN METHOD 2\n",
        "voting_clf, vectorizer = train_method2(train_data['tweet'], y_train)\n",
        "\n",
        "# Vectorize the validation and test data\n",
        "X_val_vect = vectorizer.transform(val_data['tweet'])\n",
        "X_test_vect = vectorizer.transform(test_data['tweet'])\n",
        "\n",
        "# Evaluate the Voting Classifier USING TEST METHOD 2\n",
        "test_method2(voting_clf, X_test_vect, y_test)\n",
        "\n",
        "# Make predictions on the test set\n",
        "test_predicted_labels = voting_clf.predict(X_test_vect)\n",
        "\n",
        "# Create a new DataFrame with the desired format\n",
        "output_df = test_data[['id', 'tweet', 'label']].copy()\n",
        "output_df['out_label'] = test_predicted_labels\n",
        "output_df['out_label'] = output_df['out_label'].apply(lambda x: 'NOT' if x == 0 else 'OFF')\n",
        "\n",
        "# Save the output DataFrame to a file\n",
        "model_dir = 'gdrive/MyDrive/./CE807/Assignment2/220222/models/2/100'  # Update this to your desired directory\n",
        "output_file = os.path.join(model_dir, 'output.csv')\n",
        "output_df.to_csv(output_file, index=False)\n",
        "print(f\"Output file saved at: {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WvvtYmensVN",
        "outputId": "b07f25e2-fe68-42c3-d654-201829cefca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         NOT       0.81      0.97      0.88       620\n",
            "         OFF       0.83      0.41      0.55       240\n",
            "\n",
            "    accuracy                           0.81       860\n",
            "   macro avg       0.82      0.69      0.72       860\n",
            "weighted avg       0.82      0.81      0.79       860\n",
            "\n",
            "Voting Classifier Confusion Matrix:\n",
            "[[600  20]\n",
            " [141  99]]\n",
            "Output file saved at: gdrive/MyDrive/./CE807/Assignment2/220222/models/2/100/output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "Rvu9HxgQZjKY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUu2h5WQ0l-b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}